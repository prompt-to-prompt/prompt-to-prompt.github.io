<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Prompt-to-Prompt</title>
<link href="style.css" rel="stylesheet">
<!--<script type="text/javascript" src="./ptp_files/jquery.mlens-1.0.min.js"></script>-->
<!--<script type="text/javascript" src="./ptp_files/jquery.js"></script>-->
<!--  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script>-->
  <script type="text/javascript" src="./ptp_files/cat_hat.js"></script>
</head>

<body>
<div class="content">
  <h1>Prompt-to-Prompt Image Editing with Cross-Attention Control</h1>
  <p id="authors"><a href="https://amirhertz.github.io/">Amir Hertz<sup>1,2</sup></a> <a href="https://rmokady.github.io/">Ron Mokady<sup>1,2</sup></a> <a href="jayten@google.com">Jay Tenenbaum<sup>1</sup> </a> <a href="https://kfiraberman.github.io/">Kfir Aberman<sup>1</sup></a> <a href="https://research.google/people/106214/">Yael Pritch<sup>1</sup></a> <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or<sup>1,2</sup></a><br>

  <span style="font-size: 16px"><sup>1</sup> Google Research &nbsp;&nbsp;<sup>2</sup> Tel Aviv University
  </span></p>
  <br>
  <img src="./ptp_files/teaser.png" class="teaser-gif" style="width:100%;"><br>
    <font size="+2">
          <p style="text-align: center;">
            <a href="./ptp_files/Prompt-to-Prompt_preprint.pdf" target="_blank">Paper</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://github.com/google/prompt-to-prompt/" target="_blank">Code</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Recent large-scale text-driven synthesis diffusion models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Therefore, it is only natural to build upon these synthesis models to provide text-driven image editing capabilities. However, Editing is challenging for these generative models, since an innate property of an editing technique is to preserve some content from the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive <b>prompt-to-prompt</b> editing framework, where the edits are controlled by text only. We analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we propose to control the attention maps of the edited image by injecting the attention maps of the original image along the diffusion process. Our approach enables us to monitor the synthesis process by editing the textual prompt only, paving the way to a myriad of caption-based editing applications such as localized editing by replacing a word, global editing by adding a specification, and even controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts with different text-to-image models, demonstrating high-quality synthesis and fidelity to the edited prompts.</p>
</div>
<div class="content">

  <h2>Prompt-to-Prompt Image Editing</h2>
  <p>Our method enables editing generated images by only modifying the textual prompt.

    For example, here we first generate an image from the input prompt "A cat with a hat is lying on a beach chair." using the <a href="https://imagen.research.google/" style="text-decoration: none; color: black;">Imagen</a> text-to-image diffusion model. Then, with our approach, we can easily replace the hat or the main character. </p>


  <div class="cat-hat-main">
    <img class="cat-hat-img" id="cat-hat-img" src="./ptp_files/cat_default.png">
   <div class="cat-hat-text"> "A <br><br> <span class="cat-hat-bracket"><span class="cat-hat-button_a">cat</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_a">leopard</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_a">lion</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_a">horse</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_a">turtle</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_a">elephant</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_a">giraffe</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_a">koala</span></span>
     <br><br> with a <br><br>
     <span class="cat-hat-bracket">  <span class="cat-hat-button_b">...</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">floral</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">cylinder</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">police</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">straw</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">swimming</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">pirates</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">musketeer</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">clown</span> &ensp;&ensp;&ensp;&ensp;&ensp; <span class="cat-hat-button_b">unicorn</span></span>
    <br><br> hat is lying on a beach chair." </div>
  <div>

    </div>

  </div>
     <br>

    <p>Another prompt editing example is modifying the semantic influence of specific words in the prompt over the generated image.
        Using our method, we can amplify or attenuate the "fluffiness" of the bunny doll in the image below.
       </p>
    <div class="cat-hat-main">

         <img class="cat-hat-img" id="bunny-img" src="./ptp_files/bunny_seq/fluffy_bunny_42.png"><br>
         <div class="cat-hat-text">

             <span style="display: block; text-align: center">"My <span id ="fluffy" style="background-color: #f88000;">fluffy</span> bunny doll." </span><br>
            <input type="range" min="16" max="68" value="42" class="slider" id="range_bunny">

            </div>
    </div>
  <br>
</div>
<div class="content">

  <h2>Cross-Attention Control</h2>

  <p>
    The key observation behind our method is that the spatial layout and geometry of an image depend on the cross-attention maps. Below, we show that pixels are attend more to the words that
describe them.
     </p>
    <img class="summary-img" src="./ptp_files/03_bird_and_bear.png" style="width:100%;">
    <p>
Therefore, our main idea is to inject the cross-attention maps during the diffusion
process, controlling which pixels attend to which tokens of the prompt text during which diffusion steps.
  To apply our approach to various creative editing applications, we show several methods to control the cross-attention maps through a simple and semantic interface. In the <i>word swap</i> control, we
modify a token in the prompt (e.g., “dog” to “cat”), while fixing the cross-attention maps, to preserve the scene composition. In the second, <i>prompt refinement</i> control, we add new words to the prompt and freeze
the attention to previous tokens while allowing new attention to flow to the new tokens. This enables us to perform global editing or modify a specific object.
In the third, <i>attention Re-weighting</i> control, we increase or decrease the attention weights of specified tokens.
      This results with amplification or attenuation of the semantic effect of the tokens on the generated image.
  </p>

  <img class="summary-img" src="./ptp_files/03_ca_diagram.png" style="width:100%;">



</div>
<div class="content">

  <h3>Word Swap Examples</h3>
  <p>In this case, we swap tokens of the original prompt with others, e.g., “a
basket with apples." to “a basket with oranges.”.</p>
  <img class="summary-img" src="./ptp_files/99_imagen_results_web-02.png" style="width:100%;">

  <h3>Prompt Refinement Examples</h3>
  <p>By extending the initial prompt, we perform local or global editing. </p>
   <img class="summary-img" src="./ptp_files/99_imagen_results_web-03.png" style="width:100%;">
    <h3>Attention Re-weighting Examples</h3>
   <p>By reducing or increasing the cross-attention of
specific words (marked with an arrow), we control the extent to which it influences the generation.</p>
   <img class="summary-img" src="./ptp_files/99_imagen_results_web-04.png" style="width:100%;">


<br>
  <h2>Text-to-Image Style Transfer</h2>
  <p>By adding a style description to the prompt while injecting the source attention maps, we can create various images in the new desired styles that preserve the structure of the original image.</p>
  <br>
  <img class="summary-img" src="./ptp_files/04_style_transfer_web.png" style="width:100%;"> <br>
</div>

<div class="content">
<h2>Followup Works</h2>
    <a href="https://null-text-inversion.github.io/" target="_blank">Null-text inversion for Editing Real Images using Guided Diffusion Models</a> applying Prompt-to-Prompt editing on real images by, first, inverting an input image using pivotal null-text optimization.<br><br>
    <a href="https://www.timothybrooks.com/instruct-pix2pix" target="_blank">InstructPix2Pix: Learning to Follow Image Editing Instructions</a> training an instruction to image network on synthetic data obtained by combining GPT3 and Prompt-to-Prompt on Stable Diffusion.
</div>

<div class="content">
  <h4>BibTex</h4>
  <p> @article{hertz2022prompt,<br>
  &nbsp;&nbsp;title={Prompt-to-prompt image editing with cross attention control},<br>
  &nbsp;&nbsp;author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arXiv:2208.01626},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </p>
<br>
   <h4>Acknowledgements</h4>
  <p>
    We thank Noa Glaser, Adi Zicher, Yaron Brodsky, Shlomi Fruchter and David Salesin for their valuable inputs that helped
    improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with
    their support and the pretrained models of Imagen. The website template is borrowed from <a href="https://dreambooth.github.io/" style="text-decoration: none; color: black;">DreamBooth</a>.
  </p>
</div>
</body>
</html>
